---
sidebar_position: 9
---

# AI SDK    

- [AI SDK](#ai-sdk)
  - [Stream Protocols](#stream-protocols)
  - [ğŸ“Œ Basic](#-basic)
  - [ğŸ“Œ Generative User Interfaces](#-generative-user-interfaces)
  - [ğŸ“Œ Streaming Custom Data](#-streaming-custom-data)


## Stream Protocols  

>https://sdk.vercel.ai/docs/ai-sdk-ui/stream-protocol  

stream protocol ì´ë€?  
- HTTP í”„ë¡œí† ì½œ ìœ„ì—ì„œ ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ë¥¼ ë„˜ê¸°ëŠ” ê·œì¹™. ( text stream, data stream ëª¨ë‘ í¬í•¨ )  

Text Stream Protocol
- `streamText().toTextStreamResponse();`   ì‚¬ìš©  

Data Stream Protocol
- `createDataStreamResponse` ì‚¬ìš©    

*íƒ€ì… êµ¬ë¶„ì„ í†µí•´ì„œ Stream Protocol ë‚´ Text, Data ëª¨ë‘ ë‚´ë ¤ì˜¬ ìˆ˜ ìˆë‹¤.   

*TypeID = 0,  Text Part  
- Format: 0:string\n  

*TypeID = 2,  Data Part  
- Format: 2:[{"key":"object1"},{"anotherKey":"object2"}]\n  

*TypeID = 8,  Message Annotation Part  
- Format: 8:[{"id":"message-123","other":"annotation"}]\n   

*TypeID = 3,  Error Part
- Format: 3:"error message"\n    

*TypeID = 3,  Tool Call Streaming Start Part
- Format: b:{"toolCallId":"call-456","toolName":"streaming-tool"}\n      
*TypeID = c,  Tool Call Delta Part
- Format: c:{"toolCallId":"call-456","argsTextDelta":"partial arg"}\n
*TypeID = 9,  Tool Call Part
- Format: 9:{"toolCallId":"call-123","toolName":"my-tool","args":{"some":"argument"}}\n
*TypeID = a,  Tool Result Part
- Format: a:{"toolCallId":"call-123","result":"tool output"}\n

*TypeID = e,  Finish Step Part
- Format: e:{"finishReason":"stop","usage":{"promptTokens":10,"completionTokens":20},"isContinued":false}\n
*TypeID = d,  Finish Message Part
- Format: d:{"finishReason":"stop","usage":{"promptTokens":10,"completionTokens":20}}\n

```
// ì‘ë‹µ ì˜ˆ 
// TypeID, Type delimiter(:), Text|Data Chunk, Chunk delimiter 4ê°€ì§€ íŒŒíŠ¸ë¡œ êµ¬ì„±  

2:["initialized call"]
8:[{"chunk":"123"}]
0:"Hello"
8:[{"chunk":"123"}]
0:"!"
8:[{"chunk":"123"}]
0:" How"
8:[{"chunk":"123"}]
0:" can"
8:[{"chunk":"123"}]
0:" I"
8:[{"chunk":"123"}]
0:" assist"
8:[{"chunk":"123"}]
0:" you"
8:[{"chunk":"123"}]
0:" today"
8:[{"chunk":"123"}]
0:"?"
8:[{"id":"DU5YpIiiuczDZpiN","other":"information"}]
2:["call completed"]
e:{"finishReason":"stop","usage":{"promptTokens":8,"completionTokens":9},"isContinued":false}
d:{"finishReason":"stop","usage":{"promptTokens":8,"completionTokens":9}}
```


## ğŸ“Œ Basic

```js
// app/api/chat-test/route.ts
import { openai } from "@ai-sdk/openai";
import { streamText } from "ai";

export async function POST(request: Request) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai("gpt-4o-mini"),
    system: "You are a friendly assistant!",
    messages,
    maxSteps: 5,
  });

  return result.toDataStreamResponse();
}
---
// chat-lite.tsx
"use client";

import { generateUUID } from "@/lib/utils";
import { useChat } from "ai/react";
import React from "react";

/*
[
    {
        "role": "user",
        "content": "my name is jay",
        "id": "bPclYKo27gxhIoWn",
        "createdAt": "2024-12-12T12:28:11.371Z"
    },
    {
        "id": "NPFi3J8vWw7Sa5uw", 
        "role": "assistant",
        "content": "Nice to meet you, Jay! How can I assist you today?",
        "createdAt": "2024-12-12T12:28:13.252Z",
        "revisionId": "pDmdPLtSHFUvn3Bi"
    }
]*/

const ChatLite = ({ id }: { id: string }) => {
  const {
    messages, // ì§€ê¸ˆê¹Œì§€ì˜ ëˆ„ì  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    setMessages, // ë©”ì‹œì§€ setter, (api call ì—†ìŒ)
    input, // ì‚¬ìš©ì ì…ë ¥ & setter
    setInput,
    handleSubmit, // inputì˜ ë‚´ìš©ì„ ëª¨ë¸ì— ì „ì†¡, messageê°ì²´ ì¶”ê°€, input ì´ˆê¸°í™”
    append, // message ê°ì²´ ì¶”ê°€
    isLoading,
    stop, // abort the current API
    data: streamingData, //ìµœê·¼ ì‘ë‹µë°ì´í„°ì˜ ìŠ¤íŠ¸ë¦¼
  } = useChat({
    api: "/api/chat",
    body: { id: id, modelId: "gpt-4o-mini" },
  });

  // send continue message
  console.log({ id, messages, streamingData });

  return (
    <div>
      <div>{JSON.stringify(messages)}</div>
      <input value={input} onChange={(e) => setInput(e.target.value)} />
      <button onClick={handleSubmit}>send</button>
      {/* append */}
      <div>
        <button
          onClick={() => append({ role: "user", content: "my name is jay" })}
        >
          Append suggested message!
        </button>
      </div>
    </div>
  );
};

export default ChatLite;
```

## ğŸ“Œ Generative User Interfaces

íë¦„  
1.tools ì •ì˜ í•˜ê¸° - description, parameters, execute  
- ì˜ˆ, íŠ¹ì • ìœ„ì¹˜ì˜ ë‚ ì‹œë¥¼ ë³´ì—¬ì¤˜, ì¸ìê°’:location, ì‹¤í–‰í•¨ìˆ˜ - ë‚ ì”¨ API  
2.Router Handlerì— streamText ì‘ì„±í•˜ê¸°  
3.messages ì¤‘ toolInvocations í•„ë“œë¥¼ ë³´ê³  UIë¥¼ ëœë”ë§ í•˜ê¸°  

ì œì•½
- gpt-4o-mini, gpt-4 ì´ìƒ ëª¨ë¸ ì„ íƒ  


```js
//lib/ai/tools.ts
import { tool as createTool } from "ai";
import { z } from "zod";

export const weatherTool = createTool({
  description: "Display the weather for a location",
  parameters: z.object({
    location: z.string(),
  }),
  execute: async function ({ location }) {
    await new Promise((resolve) => setTimeout(resolve, 2000));
    return { weather: "Sunny", temperature: 75, location };
  },
});

export const tools = {
  displayWeather: weatherTool,
};

---
// app/api/chat-test/route.ts
import { tools } from "@/lib/ai/tools";
import { openai } from "@ai-sdk/openai";
import { streamText } from "ai";

export async function POST(request: Request) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai("gpt-4o-mini"),
    system: "You are a friendly assistant!",
    messages,
    maxSteps: 5,
    tools,
  });

  return result.toDataStreamResponse();
}
```

```js
// components/chat-lite-ui
"use client";

import { useChat } from "ai/react";

type WeatherProps = {
  temperature: number;
  weather: string;
  location: string;
};

export const Weather = ({ temperature, weather, location }: WeatherProps) => {
  return (
    <div>
      <h2>Current Weather for {location}</h2>
      <p>Condition: {weather}</p>
      <p>Temperature: {temperature}Â°C</p>
    </div>
  );
};

const ChatLiteUI = () => {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: "/api/chat-test",
  });

  console.log("messages", messages);

  return (
    <div>
      {messages.map((message) => (
        <div key={message.id}>
          <div>{message.role === "user" ? "User: " : "AI: "}</div>
          <div>{message.content}</div>
          <div>
            {message.toolInvocations?.map((toolInvocation) => {
              const { toolName, toolCallId, state } = toolInvocation;

              if (state === "result") {
                if (toolName === "displayWeather") {
                  const { result } = toolInvocation;
                  return (
                    <div key={toolCallId}>
                      <Weather {...result} />
                    </div>
                  );
                }
              } else {
                return (
                  <div key={toolCallId}>
                    {toolName === "displayWeather" ? (
                      <div>Loading weather...</div>
                    ) : null}
                  </div>
                );
              }
            })}
          </div>
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Type a message"
        ></input>
        <button type="submit">Send</button>
      </form>
    </div>
  );
};

export default ChatLiteUI;


```


## ğŸ“Œ Streaming Custom Data

>https://sdk.vercel.ai/docs/ai-sdk-ui/streaming-data#streaming-custom-data

íë¦„
- 1.createDataStreamResponse ì„ ë¦¬í„´í•˜ë©° executeì•ˆì—ì„œ streamTextê³¼ ë¨¸ì§€í•œë‹¤.  
- `result.mergeIntoDataStream(dataStream);`  
- 2.dataStream.writeData : ìŠ¤íŠ¸ë¦¼ ë°ì´í„°, useChatì˜ dataìœ¼ë¡œ ë„˜ì–´ì˜´  
- 3.dataStream.writeMessageAnnotation : ìŠ¤íŠ¸ë¦¼ ì–´ë…¸í…Œì´ì…˜ë°ì´í„°, ì£¼ì„ê³¼ ê°™ì€ ë©”íƒ€ì •ë³´ ë„£ëŠ”ê²ƒì´ ê°€ëŠ¥.  useChatì˜ message ê°ì²´ì™€ í•¨ê»˜ ë“¤ì–´ì˜´.  

```js
// app/api/chat-test/route.ts
import { openai } from "@ai-sdk/openai";
import { generateId, createDataStreamResponse, streamText } from "ai";

export async function POST(req: Request) {
  const { messages } = await req.json();

  // immediately start streaming (solves RAG issues with status, etc.)
  return createDataStreamResponse({
    execute: (dataStream) => {
      dataStream.writeData("initialized call");

      const result = streamText({
        model: openai("gpt-4o-mini"),
        messages,
        onChunk() {
          dataStream.writeMessageAnnotation({ chunk: "123" }); // annotation ì •ë³´ëŠ” messagesì•ˆì— í¬í•¨ë˜ë©° 
        },
        onFinish() {
          // message annotation:
          dataStream.writeMessageAnnotation({
            id: generateId(), // e.g. id from saved DB record
            other: "information",
          });

          // call annotation:
          dataStream.writeData("call completed");
        },
      });

      result.mergeIntoDataStream(dataStream);
    },
    onError: (error) => {
      // Error messages are masked by default for security reasons.
      // If you want to expose the error message to the client, you can do so here:
      return error instanceof Error ? error.message : String(error);
    },
  });
}

```

```js
"use client";

import { useChat } from "ai/react";

const ChatLiteUIStreamCustom = () => {
  const {
    messages,
    input,
    handleInputChange,
    handleSubmit,
    data: streamData,
  } = useChat({
    api: "/api/chat-test",
  });

  console.log("messages", messages);
  console.log("streamData", streamData);

  return (
    <div>
      {messages.map((message) => (
        <div key={message.id}>
          <div>{message.role === "user" ? "User: " : "AI: "}</div>
          <div id="content">{message.content}</div>
          <div id="annotations">
            {message.annotations && <>{JSON.stringify(message.annotations)}</>}
          </div>
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Type a message"
        ></input>
        <button type="submit">Send</button>
      </form>
    </div>
  );
};

export default ChatLiteUIStreamCustom;

```